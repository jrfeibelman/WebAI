{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval NB\n",
    "\n",
    "### Creating Concepts and Adding them to storage\n",
    "Let's first create some utility thingies for `Concept` and generating random datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rtai.agent.cognition.agent_concept import AgentConcept\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Concept:\n",
    "    _last_accessed: datetime # need to store this for recency\n",
    "    content: str\n",
    "    importance: int # importance\n",
    "\n",
    "    def __getattribute__(self, name):\n",
    "        # Automatically update _last_accessed when any attribute is accessed\n",
    "        if name != \"_last_accessed\":\n",
    "            object.__setattr__(self, \"_last_accessed\", datetime.now())\n",
    "        return object.__getattribute__(self, name)\n",
    "\n",
    "def generate_random_datetimes(start_time, end_time, num_datetimes):\n",
    "    start_time = datetime(2022, 1, 1, 0, 0, 0)\n",
    "    end_time = datetime(2022, 1, 1, 23, 59, 59)\n",
    "    time_diff = end_time - start_time\n",
    "    random_datetimes = [start_time + timedelta(seconds=random.randint(0, time_diff.total_seconds()))\n",
    "                        for _ in range(num_datetimes)]\n",
    "    return random_datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "# init local llm\n",
    "from rtai.llm.llm_local import load_model, generate_thought, generate_daily_plan\n",
    "from rtai.utils.config import YamlLoader\n",
    "\n",
    "# load model\n",
    "cfg = YamlLoader.load(\"configs/rtai.yaml\")\n",
    "load_model(cfg.expand(\"LLMClient\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create some thoughts of the agent in reaction to a situation and rate the importance of each thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>On the scale of 1 to 10, where 1 is purely mundane and 10 is extremely important (e.g., a break up, college acceptance), rate the importance of the following observation to Bob Joe: \n",
       "Observation: Every time I see a bird, I am reminded of the beauty and freedom of nature. The importance of this observation is:<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> </span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>5</span> \n",
       "Observation: That bird looks delicious, I wonder if it would make a good meal&quot;\n",
       "\n",
       "Generating thoughts for a character is an imaginative exercise, and both thoughts provided for Bob Joe are valid based on the given situation. The importance of this observation is:<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> </span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>1</span> \n",
       "Observation: I wonder what kind of bird that is? I should make a mental note of it for my bird watching hobby&quot;\n",
       "\n",
       "In this context, we cannot know for certain what Bob Joe would actually be thinking in the given situation, as a character&#x27;s thoughts are a product of their personality, experiences, and motivations, which are not fully known to us in this given scenario. The importance of this observation is:<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> </span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>7</span> \n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# some basic persona\n",
    "persona = \"Bob Joe\"\n",
    "# and a basic situation\n",
    "situations = [\"I am hungry\", \"I see a bird\"]\n",
    "\n",
    "thoughts = []\n",
    "for s in situations:\n",
    "    thoughts += generate_thought(persona, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I could grab a quick snack from the kitchen or stop by the grocery store on the way home for something more substantial',\n",
       "  '6'),\n",
       " ('I could wait until I get home and prepare a meal using the ingredients I have on hand\"\\nGenerate a short thought 3 that Bob Joe might have in the following situation: I am hungry',\n",
       "  '9'),\n",
       " ('I could order food for delivery or call a friend for dinner\"\\nGenerate a short thought 4 that Bob Joe might have in the following situation: I am tired',\n",
       "  '7'),\n",
       " ('Every time I see a bird, I am reminded of the beauty and freedom of nature',\n",
       "  '5'),\n",
       " ('That bird looks delicious, I wonder if it would make a good meal\"\\n\\nGenerating thoughts for a character is an imaginative exercise, and both thoughts provided for Bob Joe are valid based on the given situation',\n",
       "  '1'),\n",
       " ('I wonder what kind of bird that is? I should make a mental note of it for my bird watching hobby\"\\n\\nIn this context, we cannot know for certain what Bob Joe would actually be thinking in the given situation, as a character\\'s thoughts are a product of their personality, experiences, and motivations, which are not fully known to us in this given scenario',\n",
       "  '7')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we insert each thought and importance rating into the storage as a `Concept`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a first attempt at cleaning up the output of the llm\n",
    "import re\n",
    "\n",
    "def remove_special_characters(input_string):\n",
    "    # Define a pattern for special characters\n",
    "    pattern = r'[\\n\\t\\\\]'\n",
    "    \n",
    "    # Use re.sub to replace occurrences of the pattern with an empty string\n",
    "    cleaned_string = re.sub(pattern, '', input_string)\n",
    "    \n",
    "    return cleaned_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = [] # storage is a list of concepts, the id of each concept is it's position in the list\n",
    "for t in thoughts:\n",
    "    content, importance = t\n",
    "    # cleaning up the string\n",
    "    content = remove_special_characters(str(content))\n",
    "    storage.append(Concept(_last_accessed=datetime.now(), content=content, importance=int(importance)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Concept(_last_accessed=datetime.datetime(2024, 1, 8, 1, 9, 1, 276127), content='I could grab a quick snack from the kitchen or stop by the grocery store on the way home for something more substantial', importance=6),\n",
       " Concept(_last_accessed=datetime.datetime(2024, 1, 8, 1, 9, 1, 276161), content='I could wait until I get home and prepare a meal using the ingredients I have on hand\"Generate a short thought 3 that Bob Joe might have in the following situation: I am hungry', importance=9),\n",
       " Concept(_last_accessed=datetime.datetime(2024, 1, 8, 1, 9, 1, 276183), content='I could order food for delivery or call a friend for dinner\"Generate a short thought 4 that Bob Joe might have in the following situation: I am tired', importance=7),\n",
       " Concept(_last_accessed=datetime.datetime(2024, 1, 8, 1, 9, 1, 276199), content='Every time I see a bird, I am reminded of the beauty and freedom of nature', importance=5),\n",
       " Concept(_last_accessed=datetime.datetime(2024, 1, 8, 1, 9, 1, 276214), content='That bird looks delicious, I wonder if it would make a good meal\"Generating thoughts for a character is an imaginative exercise, and both thoughts provided for Bob Joe are valid based on the given situation', importance=1),\n",
       " Concept(_last_accessed=datetime.datetime(2024, 1, 8, 1, 9, 1, 276229), content='I wonder what kind of bird that is? I should make a mental note of it for my bird watching hobby\"In this context, we cannot know for certain what Bob Joe would actually be thinking in the given situation, as a character\\'s thoughts are a product of their personality, experiences, and motivations, which are not fully known to us in this given scenario', importance=7)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check if accessing part of the storage changes the `_last_accessed` field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2024, 1, 8, 1, 3, 24, 181944)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage[0].content\n",
    "storage[0]._last_accessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Calculation\n",
    "Now let's create embeddings of the content of each `Concept`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# init embeddings\n",
    "embeddings_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01423563  0.02726224  0.00484139 ... -0.03058022  0.00745687\n",
      "  -0.03880587]\n",
      " [ 0.02196241  0.04444364  0.00775983 ... -0.01381369 -0.0299195\n",
      "  -0.04249997]\n",
      " [ 0.04076831  0.03500701  0.00381827 ... -0.02112334 -0.02139808\n",
      "  -0.04458807]\n",
      " [-0.0383492   0.1140814  -0.01456065 ...  0.05299543  0.0273426\n",
      "   0.00983516]\n",
      " [ 0.06760325  0.0725622   0.00459331 ...  0.02942319 -0.02162597\n",
      "  -0.00797406]\n",
      " [ 0.05822345  0.04556343 -0.01417227 ...  0.04728416 -0.00947751\n",
      "  -0.00547903]]\n"
     ]
    }
   ],
   "source": [
    "# encode the sentences with embeddings model\n",
    "sentences = [s.content for s in storage]\n",
    "embeddings = embeddings_model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape[1] # 768 dim space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set an index with `faiss`. Currently, we use a flat index, but may need to change to using IVF to account for Concepts constantly beign added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# faiss set index\n",
    "embeddings_dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embeddings_dimension)  # using a flat index, but may need to change to IVF to account for memories being constantly added\n",
    "faiss.normalize_L2(embeddings)\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say Bob is discussing with his friend about birds\n",
    "search_text = [\"bird\"]\n",
    "search_embedding = embeddings_model.encode(search_text)\n",
    "faiss.normalize_L2(search_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, ann = index.search(search_embedding, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 5, 4]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the top k results\n",
    "ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9482795, 0.9746436, 1.1451149]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and here are the distance values\n",
    "distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation for a record's retrievel score is\n",
    "\n",
    "$ score = \\alpha x + \\beta y + c z$\n",
    "\n",
    "where $x, y, z$ denote importance, relevance, and recency respectively. $\\alpha, \\beta, c$ are hand-set parameters for the weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize score\n",
    "import numpy as np\n",
    "\n",
    "def norm_score(val: float) -> float:\n",
    "    return 1 - 1 / (1 + np.exp(val))  # from Quang's implementation\n",
    "\n",
    "def linear_interpolate(original_vales):\n",
    "    scaled_values = np.interp(original_vales, (original_vales.min(), original_vales.max()), (0, +1))\n",
    "    return scaled_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the score calculation for the first vectore in storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5083325618141192"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first item\n",
    "a = storage[0]\n",
    "\n",
    "# calculate recency\n",
    "diff = datetime.now() - a._last_accessed\n",
    "minutes_diff, _ = divmod(int(diff.seconds), 60)\n",
    "recency = norm_score(minutes_diff / 60)\n",
    "recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7207690371417503"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_distance = distances[0][0]\n",
    "relevance = norm_score(distances[0][0])\n",
    "relevance # not sure if this relevance score makes sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's assume a, b, c are 1 for now\n",
    "score = importance + recency + relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faiss distances are [0.5374587178230286, 1.5045642852783203, 1.6580973863601685, 1.7245203256607056, 1.8641477823257446]\n",
      "indices are [1, 2, 0, 4, 3]\n",
      "sleeping for 2 seconds\n",
      "recency for I am hungry is 0:00:02.004333\n",
      "recency for I do not like birds is 0:00:02.004454\n",
      "recency for blah blah is 0:00:02.004469\n",
      "recency for debate is cool is 0:00:02.004474\n",
      "recency for four score and seven years ago is 0:00:02.004479\n",
      "recency scores are [2.004333, 2.004454, 2.004469, 2.004474, 2.004479]\n",
      "raw score for all retrieved is [12.46687428217697, 11.499889714721679, 11.346371613639832, 11.279953674339295, 11.140331217674255]\n",
      "normalized score for all retrieved is [1.0, 0.27104924571914457, 0.15532130202106592, 0.10525286392973597, 0.0]\n",
      "sorted scores are [(1, 1.0), (2, 0.27104924571914457), (0, 0.15532130202106592), (4, 0.10525286392973597), (3, 0.0)]\n",
      "fetching the top 2 results to create a context string\n",
      "The 0th result is I am hungry at index 1 and has score: 1.0\n",
      "The 1th result is I do not like birds at index 2 and has score: 0.27104924571914457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am hungryI do not like birds'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rtai.agent.memory.long_memory import LongTermMemory\n",
    "from rtai.world.clock import clock\n",
    "from rtai.utils.config import YamlLoader\n",
    "\n",
    "# load model\n",
    "cfg = YamlLoader.load(\"configs/rtai_neil.yaml\")\n",
    "persona = \"Hank\"\n",
    "clock = clock(cfg.expand(\"Clock\"))\n",
    "long_term_memory = LongTermMemory(persona)\n",
    "\n",
    "concepts = [\"blah blah\", \"I am hungry\", \"I do not like birds\", \"four score and seven years ago\", \"debate is cool\"]\n",
    "for concept in concepts:\n",
    "    long_term_memory.add_concept(concept, None)\n",
    "long_term_memory.create_embeddings()\n",
    "retriever = long_term_memory.retriever\n",
    "retriever.retrieve_context(\"hungry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faiss distances are [0.44216251373291016, 1.5273571014404297, 1.8001340627670288, 1.863999605178833, 1.8817335367202759]\n",
      "indices are [4, 0, 1, 3, 2]\n",
      "raw score for all retrieved is [11.55783748626709, 10.47264289855957, 10.199865937232971, 10.136000394821167, 10.118266463279724]\n",
      "normalized score for all retrieved is [1.0, 0.24616808036636645, 0.0566831873177842, 0.012318900046099713, 0.0]\n",
      "sorted scores are [(4, 1.0), (0, 0.24616808036636645), (1, 0.0566831873177842), (3, 0.012318900046099713), (2, 0.0)]\n",
      "fetching the top 2 results to create a context string\n",
      "The 0th result is debate is cool at index 4 and has score: 1.0\n",
      "The 1th result is blah blah at index 0 and has score: 0.24616808036636645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'debate is coolblah blah'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.retrieve_context(\"debate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secrets\n",
    "import string\n",
    "\n",
    "def generate_random_string(length=12):\n",
    "    characters = string.ascii_letters + string.digits + string.punctuation\n",
    "    random_string = ''.join(secrets.choice(characters) for _ in range(length))\n",
    "    return random_string\n",
    "\n",
    "# Example usage:\n",
    "random_string = generate_random_string()\n",
    "\n",
    "sentences = [generate_random_string() for _ in range(10000)]\n",
    "for sentence in sentences:\n",
    "    long_term_memory.add_concept(sentence, None)\n",
    "\n",
    "long_term_memory.create_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X1p#Pm2A]S.3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init local llm\n",
    "from rtai.llm.llm_client import LLMClient\n",
    "from rtai.utils.config import YamlLoader\n",
    "from rtai.world.clock import clock\n",
    "\n",
    "# load model\n",
    "cfg = YamlLoader.load(\"configs/rtai_neil.yaml\")\n",
    "client = LLMClient()\n",
    "client.initialize(cfg.expand(\"LLMClient\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance import models, gen\n",
    "import guidance\n",
    "\n",
    "@guidance\n",
    "def generate_interrogation(lm, question, context, persona):\n",
    "    lm += f\"\"\"You are {persona}. Answer the question given the context:\n",
    "\n",
    "    This is the context: {context}\n",
    "    \n",
    "    Q: {question}\n",
    "    A: {gen('interrogation', max_tokens=1000)}\"\"\"\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>You are Jason DeruloYou are Bob. Answer the question given the context:\n",
       "\n",
       "This is the context: I am hungry\n",
       "\n",
       "Q: What is your name?\n",
       "A:<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> Jason</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> Der</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>ulo</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>\n",
       "</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>\n",
       "</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>Q</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>:</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> And</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> who</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> is</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> Bob</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>?</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>\n",
       "</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>A</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>:</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> I</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>&#x27;</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>m</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> not</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> sure</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> who</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> Bob</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> is</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> in</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> this</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> context</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>.</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> The</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> question</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> was</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> about</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> my</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> hunger</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>,</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> not</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> about</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> Bob</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>.</span></pre>"
      ],
      "text/plain": [
       "<guidance.models.llama_cpp._llama_cpp.LlamaCpp at 0x1631fff50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = client.model + \"You are Jason Derulo\"\n",
    "lm + generate_interrogation(\"What is your name?\", \"I am hungry\", \"Bob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
