{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/models/lmtp/lmtp_dcinprocess.py:36: UserWarning: By default LMQL uses the 'huggyllama/llama-7b' tokenizer for all llama.cpp models. To change this, set the 'tokenizer' argument of your lmql.model(...) object.\n",
      "  warnings.warn(\"By default LMQL uses the '{}' tokenizer for all llama.cpp models. To change this, set the 'tokenizer' argument of your lmql.model(...) object.\".format(\"huggyllama/llama-7b\", UserWarning))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lmql.LLM(local:llama.cpp:/Users/nyeung/Projects/llama.cpp/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf, )"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lmql\n",
    "\n",
    "lmql.model(\"local:llama.cpp:/Users/nyeung/Projects/llama.cpp/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\", tokenizer=\"huggi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:752: OpenAILogitBiasLimitationWarning: the required logit_bias is too large to be handled by the OpenAI API and will be limited to the first 300 tokens. This can lead to the violation of the provided constraints or undesired model output. To avoid this use less broad or no constraints.\n",
      "  warnings.warn(\"the required logit_bias is too large to be handled by the OpenAI API and will be limited to the first 300 tokens. This can lead to the violation of the provided constraints or undesired model output. To avoid this use less broad or no constraints.\", category=OpenAILogitBiasLimitationWarning)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:507: OpenAIAPIWarning: OpenAI API: Underlying stream of OpenAI complete() call failed with error\n",
      "\n",
      "(\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 5.0067901611328125e-05 Average chunk time: 0.0)\", 'Stream duration:', 0.154616117477417) (<class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>)\n",
      "\n",
      "Retrying... (attempt: 0)\n",
      "  warnings.warn(f\"OpenAI API: Underlying stream of OpenAI complete() call failed with error\\n\\n{attempt.error} ({type(attempt.error)})\\n\\nRetrying... (attempt: {self.retries})\",\n",
      "OpenAIAPIWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:739: OpenAIAPIWarning: OpenAI request with ID 0 failed (timeout or other error) and will be retried\n",
      "  warnings.warn(\"OpenAI request with ID {} failed (timeout or other error) and will be retried\".format(request_id), category=OpenAIAPIWarning)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:507: OpenAIAPIWarning: OpenAI API: Underlying stream of OpenAI complete() call failed with error\n",
      "\n",
      "(\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 7.891654968261719e-05 Average chunk time: 0.0)\", 'Stream duration:', 0.0997779369354248) (<class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>)\n",
      "\n",
      "Retrying... (attempt: 1)\n",
      "  warnings.warn(f\"OpenAI API: Underlying stream of OpenAI complete() call failed with error\\n\\n{attempt.error} ({type(attempt.error)})\\n\\nRetrying... (attempt: {self.retries})\",\n",
      "OpenAIAPIWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:507: OpenAIAPIWarning: OpenAI API: Underlying stream of OpenAI complete() call failed with error\n",
      "\n",
      "(\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 9.512901306152344e-05 Average chunk time: 0.0)\", 'Stream duration:', 0.10041022300720215) (<class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>)\n",
      "\n",
      "Retrying... (attempt: 2)\n",
      "  warnings.warn(f\"OpenAI API: Underlying stream of OpenAI complete() call failed with error\\n\\n{attempt.error} ({type(attempt.error)})\\n\\nRetrying... (attempt: {self.retries})\",\n",
      "OpenAIAPIWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:507: OpenAIAPIWarning: OpenAI API: Underlying stream of OpenAI complete() call failed with error\n",
      "\n",
      "(\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 7.677078247070312e-05 Average chunk time: 0.0)\", 'Stream duration:', 0.10310816764831543) (<class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>)\n",
      "\n",
      "Retrying... (attempt: 3)\n",
      "  warnings.warn(f\"OpenAI API: Underlying stream of OpenAI complete() call failed with error\\n\\n{attempt.error} ({type(attempt.error)})\\n\\nRetrying... (attempt: {self.retries})\",\n",
      "OpenAIAPIWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:507: OpenAIAPIWarning: OpenAI API: Underlying stream of OpenAI complete() call failed with error\n",
      "\n",
      "(\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 6.67572021484375e-05 Average chunk time: 0.0)\", 'Stream duration:', 0.11861705780029297) (<class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>)\n",
      "\n",
      "Retrying... (attempt: 4)\n",
      "  warnings.warn(f\"OpenAI API: Underlying stream of OpenAI complete() call failed with error\\n\\n{attempt.error} ({type(attempt.error)})\\n\\nRetrying... (attempt: {self.retries})\",\n",
      "OpenAIAPIWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:507: OpenAIAPIWarning: OpenAI API: Underlying stream of OpenAI complete() call failed with error\n",
      "\n",
      "(\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 0.00013518333435058594 Average chunk time: 0.0)\", 'Stream duration:', 0.12337517738342285) (<class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>)\n",
      "\n",
      "Retrying... (attempt: 5)\n",
      "  warnings.warn(f\"OpenAI API: Underlying stream of OpenAI complete() call failed with error\\n\\n{attempt.error} ({type(attempt.error)})\\n\\nRetrying... (attempt: {self.retries})\",\n",
      "OpenAIAPIWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:507: OpenAIAPIWarning: OpenAI API: Underlying stream of OpenAI complete() call failed with error\n",
      "\n",
      "(\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 0.00010395050048828125 Average chunk time: 0.0)\", 'Stream duration:', 0.1014399528503418) (<class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>)\n",
      "\n",
      "Retrying... (attempt: 6)\n",
      "  warnings.warn(f\"OpenAI API: Underlying stream of OpenAI complete() call failed with error\\n\\n{attempt.error} ({type(attempt.error)})\\n\\nRetrying... (attempt: {self.retries})\",\n",
      "OpenAIAPIWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:507: OpenAIAPIWarning: OpenAI API: Underlying stream of OpenAI complete() call failed with error\n",
      "\n",
      "(\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 0.00010895729064941406 Average chunk time: 0.0)\", 'Stream duration:', 0.1214756965637207) (<class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>)\n",
      "\n",
      "Retrying... (attempt: 7)\n",
      "  warnings.warn(f\"OpenAI API: Underlying stream of OpenAI complete() call failed with error\\n\\n{attempt.error} ({type(attempt.error)})\\n\\nRetrying... (attempt: {self.retries})\",\n",
      "OpenAIAPIWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:507: OpenAIAPIWarning: OpenAI API: Underlying stream of OpenAI complete() call failed with error\n",
      "\n",
      "(\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 0.00010275840759277344 Average chunk time: 0.0)\", 'Stream duration:', 0.09845876693725586) (<class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>)\n",
      "\n",
      "Retrying... (attempt: 8)\n",
      "  warnings.warn(f\"OpenAI API: Underlying stream of OpenAI complete() call failed with error\\n\\n{attempt.error} ({type(attempt.error)})\\n\\nRetrying... (attempt: {self.retries})\",\n",
      "OpenAIAPIWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:507: OpenAIAPIWarning: OpenAI API: Underlying stream of OpenAI complete() call failed with error\n",
      "\n",
      "(\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 0.00021696090698242188 Average chunk time: 0.0)\", 'Stream duration:', 0.0985419750213623) (<class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>)\n",
      "\n",
      "Retrying... (attempt: 9)\n",
      "  warnings.warn(f\"OpenAI API: Underlying stream of OpenAI complete() call failed with error\\n\\n{attempt.error} ({type(attempt.error)})\\n\\nRetrying... (attempt: {self.retries})\",\n",
      "OpenAIAPIWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:507: OpenAIAPIWarning: OpenAI API: Underlying stream of OpenAI complete() call failed with error\n",
      "\n",
      "(\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 0.00021314620971679688 Average chunk time: 0.0)\", 'Stream duration:', 0.0980978012084961) (<class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>)\n",
      "\n",
      "Retrying... (attempt: 10)\n",
      "  warnings.warn(f\"OpenAI API: Underlying stream of OpenAI complete() call failed with error\\n\\n{attempt.error} ({type(attempt.error)})\\n\\nRetrying... (attempt: {self.retries})\",\n",
      "OpenAIAPIWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:507: OpenAIAPIWarning: OpenAI API: Underlying stream of OpenAI complete() call failed with error\n",
      "\n",
      "(\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 7.867813110351562e-05 Average chunk time: 0.0)\", 'Stream duration:', 0.10868406295776367) (<class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>)\n",
      "\n",
      "Retrying... (attempt: 11)\n",
      "  warnings.warn(f\"OpenAI API: Underlying stream of OpenAI complete() call failed with error\\n\\n{attempt.error} ({type(attempt.error)})\\n\\nRetrying... (attempt: {self.retries})\",\n",
      "OpenAIAPIWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:507: OpenAIAPIWarning: OpenAI API: Underlying stream of OpenAI complete() call failed with error\n",
      "\n",
      "(\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 7.486343383789062e-05 Average chunk time: 0.0)\", 'Stream duration:', 0.09889984130859375) (<class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>)\n",
      "\n",
      "Retrying... (attempt: 12)\n",
      "  warnings.warn(f\"OpenAI API: Underlying stream of OpenAI complete() call failed with error\\n\\n{attempt.error} ({type(attempt.error)})\\n\\nRetrying... (attempt: {self.retries})\",\n",
      "OpenAIAPIWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:507: OpenAIAPIWarning: OpenAI API: Underlying stream of OpenAI complete() call failed with error\n",
      "\n",
      "(\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 0.00011110305786132812 Average chunk time: 0.0)\", 'Stream duration:', 0.08658003807067871) (<class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>)\n",
      "\n",
      "Retrying... (attempt: 13)\n",
      "  warnings.warn(f\"OpenAI API: Underlying stream of OpenAI complete() call failed with error\\n\\n{attempt.error} ({type(attempt.error)})\\n\\nRetrying... (attempt: {self.retries})\",\n",
      "OpenAIAPIWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:507: OpenAIAPIWarning: OpenAI API: Underlying stream of OpenAI complete() call failed with error\n",
      "\n",
      "(\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 9.989738464355469e-05 Average chunk time: 0.0)\", 'Stream duration:', 0.10425710678100586) (<class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>)\n",
      "\n",
      "Retrying... (attempt: 14)\n",
      "  warnings.warn(f\"OpenAI API: Underlying stream of OpenAI complete() call failed with error\\n\\n{attempt.error} ({type(attempt.error)})\\n\\nRetrying... (attempt: {self.retries})\",\n",
      "OpenAIAPIWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:507: OpenAIAPIWarning: OpenAI API: Underlying stream of OpenAI complete() call failed with error\n",
      "\n",
      "(\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 0.00010132789611816406 Average chunk time: 0.0)\", 'Stream duration:', 0.09796524047851562) (<class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>)\n",
      "\n",
      "Retrying... (attempt: 15)\n",
      "  warnings.warn(f\"OpenAI API: Underlying stream of OpenAI complete() call failed with error\\n\\n{attempt.error} ({type(attempt.error)})\\n\\nRetrying... (attempt: {self.retries})\",\n",
      "OpenAIAPIWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:507: OpenAIAPIWarning: OpenAI API: Underlying stream of OpenAI complete() call failed with error\n",
      "\n",
      "(\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 0.00010323524475097656 Average chunk time: 0.0)\", 'Stream duration:', 0.10470914840698242) (<class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>)\n",
      "\n",
      "Retrying... (attempt: 16)\n",
      "  warnings.warn(f\"OpenAI API: Underlying stream of OpenAI complete() call failed with error\\n\\n{attempt.error} ({type(attempt.error)})\\n\\nRetrying... (attempt: {self.retries})\",\n",
      "OpenAIAPIWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:507: OpenAIAPIWarning: OpenAI API: Underlying stream of OpenAI complete() call failed with error\n",
      "\n",
      "(\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 0.00021409988403320312 Average chunk time: 0.0)\", 'Stream duration:', 0.12711882591247559) (<class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>)\n",
      "\n",
      "Retrying... (attempt: 17)\n",
      "  warnings.warn(f\"OpenAI API: Underlying stream of OpenAI complete() call failed with error\\n\\n{attempt.error} ({type(attempt.error)})\\n\\nRetrying... (attempt: {self.retries})\",\n",
      "OpenAIAPIWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:507: OpenAIAPIWarning: OpenAI API: Underlying stream of OpenAI complete() call failed with error\n",
      "\n",
      "(\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 8.797645568847656e-05 Average chunk time: 0.0)\", 'Stream duration:', 0.0908970832824707) (<class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>)\n",
      "\n",
      "Retrying... (attempt: 18)\n",
      "  warnings.warn(f\"OpenAI API: Underlying stream of OpenAI complete() call failed with error\\n\\n{attempt.error} ({type(attempt.error)})\\n\\nRetrying... (attempt: {self.retries})\",\n",
      "OpenAIAPIWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:507: OpenAIAPIWarning: OpenAI API: Underlying stream of OpenAI complete() call failed with error\n",
      "\n",
      "(\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 8.20159912109375e-05 Average chunk time: 0.0)\", 'Stream duration:', 0.0992591381072998) (<class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>)\n",
      "\n",
      "Retrying... (attempt: 19)\n",
      "  warnings.warn(f\"OpenAI API: Underlying stream of OpenAI complete() call failed with error\\n\\n{attempt.error} ({type(attempt.error)})\\n\\nRetrying... (attempt: {self.retries})\",\n",
      "OpenAIAPIWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:507: OpenAIAPIWarning: OpenAI API: Underlying stream of OpenAI complete() call failed with error\n",
      "\n",
      "(\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 9.512901306152344e-05 Average chunk time: 0.0)\", 'Stream duration:', 0.09892988204956055) (<class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>)\n",
      "\n",
      "Retrying... (attempt: 20)\n",
      "  warnings.warn(f\"OpenAI API: Underlying stream of OpenAI complete() call failed with error\\n\\n{attempt.error} ({type(attempt.error)})\\n\\nRetrying... (attempt: {self.retries})\",\n",
      "OpenAIAPIWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "MaximumRetriesExceeded",
     "evalue": "Maximum retries exceeded (21) with error <class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>: (\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 9.512901306152344e-05 Average chunk time: 0.0)\", 'Stream duration:', 0.09892988204956055)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMaximumRetriesExceeded\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''lmql\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    # top-level strings are prompts\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    \"Q: What is the answer to life, the \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    return NUM\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# so from Python, you can just do this\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m meaning_of_life() \u001b[38;5;66;03m# 42\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/lmql_runtime.py:230\u001b[0m, in \u001b[0;36mLMQLQueryFunction.__acall__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m         PromptInterpreter\u001b[38;5;241m.\u001b[39mmain \u001b[38;5;241m=\u001b[39m interpreter\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# execute main prompt\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m interpreter\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfct, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mquery_kwargs)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m PromptInterpreter\u001b[38;5;241m.\u001b[39mmain \u001b[38;5;241m==\u001b[39m interpreter:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/tracing/tracer.py:240\u001b[0m, in \u001b[0;36mtrace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m     tracer \u001b[38;5;241m=\u001b[39m Tracer(name)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextTracer(tracer):\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fct(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/interpreter.py:1070\u001b[0m, in \u001b[0;36mPromptInterpreter.run\u001b[0;34m(self, fct, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m average_step_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 1070\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m decoder_fct(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecoder_args):\n\u001b[1;32m   1071\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m debug_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_step)\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/dclib/decoders.py:21\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(prompt_ids, n, max_len, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m h\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(h) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 21\u001b[0m     h \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;01mawait\u001b[39;00m model\u001b[38;5;241m.\u001b[39margmax(h, noscore\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     22\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m model\u001b[38;5;241m.\u001b[39mrewrite(h, noscore\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m     h, done \u001b[38;5;241m=\u001b[39m (h \u001b[38;5;241m+\u001b[39m done)\u001b[38;5;241m.\u001b[39mseparate_by(dc\u001b[38;5;241m.\u001b[39mlogical_not(dc\u001b[38;5;241m.\u001b[39meos), dc\u001b[38;5;241m.\u001b[39mlt(max_len))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/dclib/dclib_cache.py:277\u001b[0m, in \u001b[0;36mCachedDcModel.argmax\u001b[0;34m(self, arr, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;66;03m# read full result from cache\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m arr\u001b[38;5;241m.\u001b[39maelement_wise(op_argmax)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/dclib/dclib_array.py:318\u001b[0m, in \u001b[0;36mDataArray.aelement_wise\u001b[0;34m(self, op, *args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mop_with_path\u001b[39m(path, element, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m path, \u001b[38;5;28;01mawait\u001b[39;00m op(element, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 318\u001b[0m result_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39m[op_with_path(path, seqs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m path, seqs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequences\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataArray(\u001b[38;5;28mdict\u001b[39m(result_items))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/dclib/dclib_array.py:317\u001b[0m, in \u001b[0;36mDataArray.aelement_wise.<locals>.op_with_path\u001b[0;34m(path, element, *args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mop_with_path\u001b[39m(path, element, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m path, \u001b[38;5;28;01mawait\u001b[39;00m op(element, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/dclib/dclib_cache.py:256\u001b[0m, in \u001b[0;36mCachedDcModel.argmax.<locals>.op_argmax\u001b[0;34m(seqs)\u001b[0m\n\u001b[1;32m    254\u001b[0m non_cached \u001b[38;5;241m=\u001b[39m [s \u001b[38;5;28;01mfor\u001b[39;00m s, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(seqs, cached_tokens) \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# generator over new results\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m non_cached_argmax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m((\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelegate\u001b[38;5;241m.\u001b[39margmax(DataArray(non_cached), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\u001b[38;5;241m.\u001b[39mitems())                \n\u001b[1;32m    258\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# put new results in cache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/openai_integration.py:527\u001b[0m, in \u001b[0;36mDclibOpenAiModel.argmax\u001b[0;34m(self, sequences, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margmax\u001b[39m(\u001b[38;5;28mself\u001b[39m, sequences, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(sequences, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/openai_integration.py:645\u001b[0m, in \u001b[0;36mDclibOpenAiModel.sample\u001b[0;34m(self, sequences, num_samples, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [s\u001b[38;5;241m.\u001b[39mmake_successors(next_token_ids[i], next_token_scores[i], logits\u001b[38;5;241m=\u001b[39mlogits[i], \n\u001b[1;32m    643\u001b[0m         user_data\u001b[38;5;241m=\u001b[39msuccessor_user_data(continuation_buffers[i], \u001b[38;5;28mlen\u001b[39m(next_token_ids[i]), edge_type_user_data)) \u001b[38;5;28;01mfor\u001b[39;00m i, s, edge_type_user_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seqs)), seqs, edge_type_populated_user_data)]\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mtimer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m sequences\u001b[38;5;241m.\u001b[39maelement_wise(op_sample)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/dclib/dclib_array.py:318\u001b[0m, in \u001b[0;36mDataArray.aelement_wise\u001b[0;34m(self, op, *args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mop_with_path\u001b[39m(path, element, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m path, \u001b[38;5;28;01mawait\u001b[39;00m op(element, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 318\u001b[0m result_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39m[op_with_path(path, seqs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m path, seqs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequences\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataArray(\u001b[38;5;28mdict\u001b[39m(result_items))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/dclib/dclib_array.py:317\u001b[0m, in \u001b[0;36mDataArray.aelement_wise.<locals>.op_with_path\u001b[0;34m(path, element, *args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mop_with_path\u001b[39m(path, element, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m path, \u001b[38;5;28;01mawait\u001b[39;00m op(element, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/openai_integration.py:560\u001b[0m, in \u001b[0;36mDclibOpenAiModel.sample.<locals>.op_sample\u001b[0;34m(seqs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     sampling_modes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemperature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-sample-id-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m32\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seqs))]\n\u001b[1;32m    558\u001b[0m     edge_type_populated_user_data \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdc-edge-type\u001b[39m\u001b[38;5;124m\"\u001b[39m: sm} \u001b[38;5;28;01mfor\u001b[39;00m sm \u001b[38;5;129;01min\u001b[39;00m sampling_modes]\n\u001b[0;32m--> 560\u001b[0m completions: List[CompletionResult] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_buffer(seqs, logprobs\u001b[38;5;241m=\u001b[39mnum_samples, sampling_modes\u001b[38;5;241m=\u001b[39msampling_modes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    562\u001b[0m next_token_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    563\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/openai_integration.py:449\u001b[0m, in \u001b[0;36mDclibOpenAiModel.completion_buffer\u001b[0;34m(self, seqs, temperature, sampling_modes, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m completion_result\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mempty(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompletion result is empty on arrival: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m([\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_call\u001b[38;5;241m.\u001b[39minput_ids)]))\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m completion_result\n\u001b[0;32m--> 449\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39m[get_buffer(i, s) \u001b[38;5;28;01mfor\u001b[39;00m i, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seqs)])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/openai_integration.py:442\u001b[0m, in \u001b[0;36mDclibOpenAiModel.completion_buffer.<locals>.get_buffer\u001b[0;34m(i, s)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# eagerly expand and cache full completion if a cache_delegate is available\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_delegate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 442\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpand_and_cache(s, completion_result, \n\u001b[1;32m    443\u001b[0m                                 sampling_modes[i],\n\u001b[1;32m    444\u001b[0m                                 logprobs\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m completion_result\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mempty(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompletion result is empty on arrival: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m([\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_call\u001b[38;5;241m.\u001b[39minput_ids)]))\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion_result\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/openai_integration.py:454\u001b[0m, in \u001b[0;36mDclibOpenAiModel.expand_and_cache\u001b[0;34m(self, s, completion_result, sampling_mode, logprobs)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexpand_and_cache\u001b[39m(\u001b[38;5;28mself\u001b[39m, s: DecoderSequence, completion_result: CompletionResult, sampling_mode, logprobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;66;03m# wait at least for the first completion so \u001b[39;00m\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# the cache is guaranteed to be ahead\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m     _res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m completion_result\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoken_stream\u001b[39m():\n\u001b[1;32m    457\u001b[0m         \u001b[38;5;28;01mnonlocal\u001b[39;00m sampling_mode, s, completion_result\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:250\u001b[0m, in \u001b[0;36mresponse_buffer_slice.get\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mget(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlower)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:334\u001b[0m, in \u001b[0;36mresponse_buffer.get\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_tokens \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m i \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m anext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append(chunk)\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:516\u001b[0m, in \u001b[0;36mResponseStreamSliceIterator.__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot recover from stream error without a configured tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m attempt\u001b[38;5;241m.\u001b[39merror\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecover()\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsumed_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:425\u001b[0m, in \u001b[0;36mResponseStreamSliceIterator.recover\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice \u001b[38;5;241m=\u001b[39m new_slice\n\u001b[1;32m    424\u001b[0m \u001b[38;5;66;03m# otherwise the chunking aligns with the old stream, so we return the next chunk\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:516\u001b[0m, in \u001b[0;36mResponseStreamSliceIterator.__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot recover from stream error without a configured tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m attempt\u001b[38;5;241m.\u001b[39merror\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecover()\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsumed_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:425\u001b[0m, in \u001b[0;36mResponseStreamSliceIterator.recover\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice \u001b[38;5;241m=\u001b[39m new_slice\n\u001b[1;32m    424\u001b[0m \u001b[38;5;66;03m# otherwise the chunking aligns with the old stream, so we return the next chunk\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n",
      "    \u001b[0;31m[... skipping similar frames: ResponseStreamSliceIterator.__anext__ at line 516 (17 times), ResponseStreamSliceIterator.recover at line 425 (17 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:516\u001b[0m, in \u001b[0;36mResponseStreamSliceIterator.__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot recover from stream error without a configured tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m attempt\u001b[38;5;241m.\u001b[39merror\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecover()\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsumed_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:425\u001b[0m, in \u001b[0;36mResponseStreamSliceIterator.recover\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice \u001b[38;5;241m=\u001b[39m new_slice\n\u001b[1;32m    424\u001b[0m \u001b[38;5;66;03m# otherwise the chunking aligns with the old stream, so we return the next chunk\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/embeddings_env/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:512\u001b[0m, in \u001b[0;36mResponseStreamSliceIterator.__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# if we have exceeded the maximum number of retries, raise the error\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretries \u001b[38;5;241m>\u001b[39m attempt\u001b[38;5;241m.\u001b[39mmaximum_retries:\n\u001b[0;32m--> 512\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaximumRetriesExceeded(attempt\u001b[38;5;241m.\u001b[39merror, retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretries)\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot recover from stream error without a configured tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mMaximumRetriesExceeded\u001b[0m: Maximum retries exceeded (21) with error <class 'lmql.runtime.bopenai.openai_api.OpenAIStreamError'>: (\"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys. (after receiving 0 chunks. Current chunk time: 9.512901306152344e-05 Average chunk time: 0.0)\", 'Stream duration:', 0.09892988204956055)"
     ]
    }
   ],
   "source": [
    "@lmql.query\n",
    "async def meaning_of_life():\n",
    "    '''lmql\n",
    "    # top-level strings are prompts\n",
    "    \"Q: What is the answer to life, the \\\n",
    "     universe and everything?\"\n",
    "\n",
    "    # generation via (constrained) variables\n",
    "    \"A: [ANSWER]\" where \\\n",
    "        len(ANSWER) < 120 and STOPS_AT(ANSWER, \".\")\n",
    "\n",
    "    # results are directly accessible\n",
    "    print(\"LLM returned\", ANSWER)\n",
    "\n",
    "    # use typed variables for guaranteed \n",
    "    # output format\n",
    "    \"The answer is [NUM: int]\"\n",
    "\n",
    "    # query programs are just functions \n",
    "    return NUM\n",
    "    '''\n",
    "\n",
    "# so from Python, you can just do this\n",
    "await meaning_of_life() # 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meaning_of_life' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmeaning_of_life\u001b[49m() \u001b[38;5;66;03m# 42\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'meaning_of_life' is not defined"
     ]
    }
   ],
   "source": [
    "meaning_of_life() # 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embeddings_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
