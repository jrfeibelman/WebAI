{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/jfeibs/JRF/repos/WebAI/rtai/../models/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: system memory used  = 4165.48 MiB\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 76.19 MiB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n8 threads: 2 min 5 sec\\n4 threads: 1 min 37 sec\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from os import getcwd\n",
    "\n",
    "# llm = Llama(model_path=\"%s/../models/mistral-7b-instruct-v0.1.Q5_K_S.gguf\" % getcwd(), n_threads=4)\n",
    "llm = Llama(model_path=\"%s/../models/mistral-7b-instruct-v0.1.Q4_K_M.gguf\" % getcwd())\n",
    "\"\"\"\n",
    "8 threads: 2 min 5 sec\n",
    "4 threads: 1 min 37 sec\n",
    "\"\"\"\n",
    "# llm = Llama(model_path=\"%s/../models/llama-2-7b.Q4_K_M.gguf\" % getcwd(), n_threads=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-83d4d909-4f45-4f84-851e-f3e0ffb3f16e', 'object': 'text_completion', 'created': 1703390409, 'model': '/Users/jfeibs/JRF/repos/WebAI/rtai/../models/mistral-7b-instruct-v0.1.Q4_K_M.gguf', 'choices': [{'text': ' crust:\\n\\nIngredients:\\n- 3 cups all-purpose flour\\n- 1 package active dry yeast\\n- 2 tablespoons olive oil\\n- 2 teaspoons salt\\n- 1 cup warm water\\n\\nInstructions:\\n1. Begin by combining the flour, yeast, salt and olive oil in a large bowl. Mix everything together until it forms a rough dough.\\n\\n2. Slowly pour in the warm water while stirring the dough. Be sure to mix everything well, so that there are no lumps remaining. \\n\\n3. Knead the dough on a floured surface for about 10 minutes. This will help develop gluten and make the crust more elastic. If the dough is too sticky, add more flour as needed.\\n\\n4. Place the dough in a greased bowl, cover with a clean cloth or plastic wrap, and allow it to rise in a warm, draft-free environment for about 1 hour. This will allow the yeast to ferment and produce carbon dioxide, causing the dough to expand. \\n\\n5. Once the dough has risen, gently punch it down and divide it into the desired number of portions. Roll out each portion on a floured surface until it reaches your desired thickness.\\n\\n6. Preheat your oven to the highest temperature possible, usually around 500°F or 260°C. Place the pizza crust on a greased or parchment-lined baking sheet or pizza stone.\\n\\n7. Add your favorite toppings and bake for 10-15 minutes, until the crust is golden brown and the cheese is melted and bubbly.\\n\\n8. Remove from oven, let it cool for a few minutes, slice and serve hot. Enjoy!', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 13, 'completion_tokens': 393, 'total_tokens': 406}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2011.22 ms\n",
      "llama_print_timings:      sample time =     129.41 ms /   394 runs   (    0.33 ms per token,  3044.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2010.91 ms /    13 tokens (  154.69 ms per token,     6.46 tokens per second)\n",
      "llama_print_timings:        eval time =   93242.07 ms /   393 runs   (  237.26 ms per token,     4.21 tokens per second)\n",
      "llama_print_timings:       total time =   96726.29 ms\n"
     ]
    }
   ],
   "source": [
    "answer = llm.create_completion('Write a step-by-step recipe to make a pizza', max_tokens=512)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " crust:\n",
      "\n",
      "Ingredients:\n",
      "- 3 cups all-purpose flour\n",
      "- 1 package active dry yeast\n",
      "- 2 tablespoons olive oil\n",
      "- 2 teaspoons salt\n",
      "- 1 cup warm water\n",
      "\n",
      "Instructions:\n",
      "1. Begin by combining the flour, yeast, salt and olive oil in a large bowl. Mix everything together until it forms a rough dough.\n",
      "\n",
      "2. Slowly pour in the warm water while stirring the dough. Be sure to mix everything well, so that there are no lumps remaining. \n",
      "\n",
      "3. Knead the dough on a floured surface for about 10 minutes. This will help develop gluten and make the crust more elastic. If the dough is too sticky, add more flour as needed.\n",
      "\n",
      "4. Place the dough in a greased bowl, cover with a clean cloth or plastic wrap, and allow it to rise in a warm, draft-free environment for about 1 hour. This will allow the yeast to ferment and produce carbon dioxide, causing the dough to expand. \n",
      "\n",
      "5. Once the dough has risen, gently punch it down and divide it into the desired number of portions. Roll out each portion on a floured surface until it reaches your desired thickness.\n",
      "\n",
      "6. Preheat your oven to the highest temperature possible, usually around 500°F or 260°C. Place the pizza crust on a greased or parchment-lined baking sheet or pizza stone.\n",
      "\n",
      "7. Add your favorite toppings and bake for 10-15 minutes, until the crust is golden brown and the cheese is melted and bubbly.\n",
      "\n",
      "8. Remove from oven, let it cool for a few minutes, slice and serve hot. Enjoy!\n"
     ]
    }
   ],
   "source": [
    "print(answer['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"1\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        print\n",
    "        str\n",
    "        \"\"\"\n",
    "        return \"2\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        display\n",
    "        \"\"\"\n",
    "        return \"3\"\n",
    "    \n",
    "t = Tester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tester' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tester' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "t.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./rtai'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from os import environ\n",
    "load_dotenv()\n",
    "environ['RTAI_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting marvin\n",
      "  Downloading marvin-1.5.6-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting beautifulsoup4>=4.12.2 (from marvin)\n",
      "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fastapi>=0.98.0 (from marvin)\n",
      "  Downloading fastapi-0.108.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /opt/homebrew/Caskroom/miniforge/base/envs/web_ai/lib/python3.12/site-packages (from marvin) (0.26.0)\n",
      "Collecting jinja2>=3.1.2 (from marvin)\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting jsonpatch>=1.33 (from marvin)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting openai<1.0.0,>=0.27.8 (from marvin)\n",
      "  Using cached openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pydantic>=1.10.7 in /opt/homebrew/Caskroom/miniforge/base/envs/web_ai/lib/python3.12/site-packages (from pydantic[dotenv]>=1.10.7->marvin) (2.5.3)\n",
      "Collecting rich>=12 (from marvin)\n",
      "  Using cached rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tiktoken>=0.4.0 (from marvin)\n",
      "  Downloading tiktoken-0.5.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting typer>=0.9.0 (from marvin)\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Collecting tzdata>=2023.3 (from marvin)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Collecting uvicorn>=0.22.0 (from marvin)\n",
      "  Downloading uvicorn-0.25.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4>=4.12.2->marvin)\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting starlette<0.33.0,>=0.29.0 (from fastapi>=0.98.0->marvin)\n",
      "  Downloading starlette-0.32.0.post1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/Caskroom/miniforge/base/envs/web_ai/lib/python3.12/site-packages (from fastapi>=0.98.0->marvin) (4.9.0)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/Caskroom/miniforge/base/envs/web_ai/lib/python3.12/site-packages (from httpx>=0.24.1->marvin) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/Caskroom/miniforge/base/envs/web_ai/lib/python3.12/site-packages (from httpx>=0.24.1->marvin) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Caskroom/miniforge/base/envs/web_ai/lib/python3.12/site-packages (from httpx>=0.24.1->marvin) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/homebrew/Caskroom/miniforge/base/envs/web_ai/lib/python3.12/site-packages (from httpx>=0.24.1->marvin) (3.6)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/Caskroom/miniforge/base/envs/web_ai/lib/python3.12/site-packages (from httpx>=0.24.1->marvin) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/Caskroom/miniforge/base/envs/web_ai/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.24.1->marvin) (0.14.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=3.1.2->marvin)\n",
      "  Using cached MarkupSafe-2.1.3-cp312-cp312-macosx_10_9_universal2.whl.metadata (2.9 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch>=1.33->marvin)\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting requests>=2.20 (from openai<1.0.0,>=0.27.8->marvin)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniforge/base/envs/web_ai/lib/python3.12/site-packages (from openai<1.0.0,>=0.27.8->marvin) (4.66.1)\n",
      "Collecting aiohttp (from openai<1.0.0,>=0.27.8->marvin)\n",
      "  Downloading aiohttp-3.9.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/Caskroom/miniforge/base/envs/web_ai/lib/python3.12/site-packages (from pydantic>=1.10.7->pydantic[dotenv]>=1.10.7->marvin) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/homebrew/Caskroom/miniforge/base/envs/web_ai/lib/python3.12/site-packages (from pydantic>=1.10.7->pydantic[dotenv]>=1.10.7->marvin) (2.14.6)\n",
      "\u001b[33mWARNING: pydantic 2.5.3 does not provide the extra 'dotenv'\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting markdown-it-py>=2.2.0 (from rich>=12->marvin)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Caskroom/miniforge/base/envs/web_ai/lib/python3.12/site-packages (from rich>=12->marvin) (2.17.2)\n",
      "Collecting regex>=2022.1.18 (from tiktoken>=0.4.0->marvin)\n",
      "  Downloading regex-2023.12.25-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting click<9.0.0,>=7.1.1 (from typer>=0.9.0->marvin)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=12->marvin)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.20->openai<1.0.0,>=0.27.8->marvin)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.20->openai<1.0.0,>=0.27.8->marvin)\n",
      "  Using cached urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->openai<1.0.0,>=0.27.8->marvin)\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->openai<1.0.0,>=0.27.8->marvin)\n",
      "  Downloading multidict-6.0.4.tar.gz (51 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp->openai<1.0.0,>=0.27.8->marvin)\n",
      "  Downloading yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->openai<1.0.0,>=0.27.8->marvin)\n",
      "  Downloading frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->openai<1.0.0,>=0.27.8->marvin)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading marvin-1.5.6-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.108.0-py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "Using cached rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "Downloading tiktoken-0.5.2-cp312-cp312-macosx_11_0_arm64.whl (953 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.6/953.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.25.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached MarkupSafe-2.1.3-cp312-cp312-macosx_10_9_universal2.whl (17 kB)\n",
      "Downloading regex-2023.12.25-cp312-cp312-macosx_11_0_arm64.whl (292 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.2/292.2 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Downloading starlette-0.32.0.post1-py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.0/70.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.1-cp312-cp312-macosx_11_0_arm64.whl (388 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Downloading frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "Downloading yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl (79 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.4/79.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: multidict\n",
      "  Building wheel for multidict (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for multidict: filename=multidict-6.0.4-cp312-cp312-macosx_11_0_arm64.whl size=27126 sha256=02b060d23a62c69564d08cac5eeb69f815bdc73aa831816bee54fffb4ef6e945\n",
      "  Stored in directory: /Users/nyeung/Library/Caches/pip/wheels/f6/d8/ff/3c14a64b8f2ab1aa94ba2888f5a988be6ab446ec5c8d1a82da\n",
      "Successfully built multidict\n",
      "Installing collected packages: urllib3, tzdata, soupsieve, regex, multidict, mdurl, MarkupSafe, jsonpointer, frozenlist, click, charset-normalizer, attrs, yarl, uvicorn, typer, starlette, requests, markdown-it-py, jsonpatch, jinja2, beautifulsoup4, aiosignal, tiktoken, rich, fastapi, aiohttp, openai, marvin\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.6.1\n",
      "    Uninstalling openai-1.6.1:\n",
      "      Successfully uninstalled openai-1.6.1\n",
      "Successfully installed MarkupSafe-2.1.3 aiohttp-3.9.1 aiosignal-1.3.1 attrs-23.1.0 beautifulsoup4-4.12.2 charset-normalizer-3.3.2 click-8.1.7 fastapi-0.108.0 frozenlist-1.4.1 jinja2-3.1.2 jsonpatch-1.33 jsonpointer-2.4 markdown-it-py-3.0.0 marvin-1.5.6 mdurl-0.1.2 multidict-6.0.4 openai-0.28.1 regex-2023.12.25 requests-2.31.0 rich-13.7.0 soupsieve-2.5 starlette-0.32.0.post1 tiktoken-0.5.2 typer-0.9.0 tzdata-2023.3 urllib3-2.1.0 uvicorn-0.25.0 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip3 install marvin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic\n",
    "\n",
    "class City(pydantic.BaseModel):\n",
    "    '''\n",
    "        A model to represent a city.\n",
    "    '''\n",
    "\n",
    "    text: str = pydantic.Field(\n",
    "        description = 'The city name as it appears'\n",
    "    )\n",
    "\n",
    "    inferred_city: str = pydantic.Field(\n",
    "            description = 'The inferred and normalized city name.'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'marvin.openai' has no attribute 'api_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmarvin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prompt_fn\n\u001b[1;32m      3\u001b[0m \u001b[38;5;129m@prompt_fn\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_cities\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[City]:\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m        Expertly deduce and infer all cities from the follwing text: {{ text }}\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/web_ai/lib/python3.12/site-packages/marvin/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msettings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m settings\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     ai_classifier,\n\u001b[1;32m      5\u001b[0m     ai_fn,\n\u001b[1;32m      6\u001b[0m     ai_model,\n\u001b[1;32m      7\u001b[0m     AIApplication,\n\u001b[1;32m      8\u001b[0m     AIFunction,\n\u001b[1;32m      9\u001b[0m     AIModel,\n\u001b[1;32m     10\u001b[0m     AIModelFactory,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version \u001b[38;5;28;01mas\u001b[39;00m __version__\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/web_ai/lib/python3.12/site-packages/marvin/components/__init__.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai_model_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AIModelFactory\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AIModel\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai_classifier\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ai_classifier\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/web_ai/lib/python3.12/site-packages/marvin/components/ai_classifier.py:88\u001b[0m\n\u001b[1;32m     84\u001b[0m     instructions: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     mode: Optional[Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mAIEnumMeta\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mEnumMeta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;43;03m    A metaclass for the AIEnum class.\u001b[39;49;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43m__metadata__\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mAIEnumMetaData\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/web_ai/lib/python3.12/site-packages/marvin/components/ai_classifier.py:97\u001b[0m, in \u001b[0;36mAIEnumMeta\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAIEnumMeta\u001b[39;00m(EnumMeta):\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    A metaclass for the AIEnum class.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     __metadata__ \u001b[38;5;241m=\u001b[39m \u001b[43mAIEnumMetaData\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28mcls\u001b[39m: Self,\n\u001b[1;32m    101\u001b[0m         value: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs: Any,\n\u001b[1;32m    114\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtype\u001b[39m[Enum]:\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__metadata__ \u001b[38;5;241m=\u001b[39m AIEnumMetaData(\n\u001b[1;32m    116\u001b[0m             model\u001b[38;5;241m=\u001b[39mChatCompletion(model\u001b[38;5;241m=\u001b[39mmodel, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs),\n\u001b[1;32m    117\u001b[0m             ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    118\u001b[0m             instructions\u001b[38;5;241m=\u001b[39minstructions,\n\u001b[1;32m    119\u001b[0m             mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    120\u001b[0m         )\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/web_ai/lib/python3.12/site-packages/marvin/core/ChatCompletion/__init__.py:50\u001b[0m, in \u001b[0;36mChatCompletion\u001b[0;34m(model, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure_openai\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproviders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIChatCompletion\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOpenAIChatCompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproviders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manthropic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnthropicChatCompletion\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/web_ai/lib/python3.12/site-packages/marvin/core/ChatCompletion/providers/openai.py:123\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.__init__\u001b[0;34m(self, provider, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, provider: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(defaults\u001b[38;5;241m=\u001b[39m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenai\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m|\u001b[39m kwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/web_ai/lib/python3.12/site-packages/marvin/settings.py:174\u001b[0m, in \u001b[0;36mSettings.get_defaults\u001b[0;34m(self, provider)\u001b[0m\n\u001b[1;32m    172\u001b[0m response: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manthropic\u001b[38;5;241m.\u001b[39mget_defaults(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/web_ai/lib/python3.12/site-packages/marvin/settings.py:57\u001b[0m, in \u001b[0;36mOpenAISettings.get_defaults\u001b[0;34m(self, settings)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mapi_key:\n\u001b[1;32m     56\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mapi_key\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmarvin_openai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_key\u001b[49m:\n\u001b[1;32m     58\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m marvin_openai\u001b[38;5;241m.\u001b[39mapi_key\n\u001b[1;32m     59\u001b[0m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mllm_temperature\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'marvin.openai' has no attribute 'api_key'"
     ]
    }
   ],
   "source": [
    "from marvin import prompt_fn\n",
    "\n",
    "@prompt_fn\n",
    "def get_cities(text: str) -> list[City]:\n",
    "    '''\n",
    "        Expertly deduce and infer all cities from the follwing text: {{ text }}\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"[As Batman] I'm the Dark Knight, protector of Gotham City. An unyielding guardian against crime and injustice. With my hi-tech gadgets and stealth abilities, I strike fear into the hearts of criminals.\\n\\n### Introduce: Dr. Amelia Hart\\n[As Batman] Meet Dr. Amelia Hart, a brilliant scientist and forensic expert. Her sharp mind and unwavering dedication to truth have made her an invaluable asset to the Gotham Police Department.\\n\\n### Introduce: Selina Kyle (Catwoman)\\n[As Batman] This is Selina Kyle, also known as Catwoman. She's a master thief with a complex past. Though she often finds herself on the wrong side of the law, I've come to respect her cunning and resourcefulness.\\n\\n### Introduce: Jim Gordon\\n[As Batman] Officer Jim Gordon is a dedicated cop and my trusted ally. His unwavering commitment to justice and ability to navigate Gotham's political landscape make him an essential partner in our fight against crime.\\n\\n### Introduce: Edward Nygma (The Riddler)\\n[As Batman] And this is Edward Nygma, a former colleague turned criminal mastermind known as The Riddler. He delights in testing my intellect with elaborate puzzles and schemes, but make no mistake - beneath the enigma lies a dangerous madman.\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"local-model\", # this field is currently unused\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a storyteller. Generate short descriptions of the following characters\"},\n",
    "        {\"role\": \"user\", \"content\": \"Introduce yourself as batman.\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    ")\n",
    "print(str(response.choices[0].message).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rtai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n",
      "File \u001b[0;32m~/Projects/WebAI/rtai/utils/config.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m path, environ\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mcompile\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrtai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cache\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrtai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m error\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mConfig\u001b[39;00m(Cache):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rtai'"
     ]
    }
   ],
   "source": [
    "from utils.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class CustomFormatter(logging.Formatter):\n",
    "    grey = \"\\\\x1b[38;21m\"\n",
    "    yellow = \"\\\\x1b[33;21m\"\n",
    "    red = \"\\\\x1b[31;21m\"\n",
    "    bold_red = \"\\\\x1b[31;1m\"\n",
    "    reset = \"\\\\x1b[0m\"\n",
    "    format = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s (%(filename)s:%(lineno)d)\"\n",
    "\n",
    "    FORMATS = {\n",
    "        logging.DEBUG: grey + format + reset,\n",
    "        logging.INFO: grey + format + reset,\n",
    "        logging.WARNING: yellow + format + reset,\n",
    "        logging.ERROR: red + format + reset,\n",
    "        logging.CRITICAL: bold_red + format + reset\n",
    "    }\n",
    "\n",
    "    def format(self, record):\n",
    "        log_fmt = self.FORMATS.get(record.levelno)\n",
    "        formatter = logging.Formatter(log_fmt)\n",
    "        return formatter.format(record)\n",
    "    \n",
    "logging.info(\"TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-11T10:09\n",
      "2023-12-11 10:09:00\n"
     ]
    }
   ],
   "source": [
    "from numpy import datetime64\n",
    "from datetime import datetime\n",
    "from time import perf_counter\n",
    "# dt = datetime64(\"2023-12-11 10:09\")\n",
    "data = datetime64(\"2023-12-11 10:09\")\n",
    "data2 = datetime.strptime('2023-12-11 10:09', \"%Y-%m-%d %H:%M\")\n",
    "print(data)\n",
    "print(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'05:09'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(data.astype('datetime64[s]')).split('T')[1][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-11  -->  Time: [217.26718201534823] ms\n"
     ]
    }
   ],
   "source": [
    "start = perf_counter()\n",
    "# d = [str(data.astype('datetime64[T]')) for i in range(1000)]\n",
    "d = [str(data.astype('datetime64[D]')) for i in range(100000)]\n",
    "print(\"%s  -->  Time: [%s] ms\" % (d[0], 1000 * (perf_counter() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:09:00  -->  Time: [421.6937150049489] ms\n"
     ]
    }
   ],
   "source": [
    "start = perf_counter()\n",
    "d2 = [str(data2.strftime('%H:%M')) for i in range(100000)]\n",
    "print(\"%s  -->  Time: [%s] ms\" % (d2[0], 1000 * (perf_counter() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-03T00:28:33  -->  Time: [492.6772819890175] ms\n",
      "2024-01-03T00:28:34.104052  -->  Time: [2881.395889999112] ms\n"
     ]
    }
   ],
   "source": [
    "# start = perf_counter()\n",
    "# d = [datetime64('now') for i in range(1000000)]\n",
    "# print(\"%s  -->  Time: [%s] ms\" % (d[0], 1000 * (perf_counter() - start)))\n",
    "# start = perf_counter()\n",
    "# d = [datetime64(datetime.utcnow()) for i in range(1000000)]\n",
    "# print(\"%s  -->  Time: [%s] ms\" % (d[0], 1000 * (perf_counter() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1900-01-01 02:32:00 PM'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "d = datetime.strptime('14:32', \"%H:%M\")\n",
    "d.strftime(\"%Y-%m-%d %I:%M:%S %p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
