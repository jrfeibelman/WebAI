{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/jfeibs/JRF/repos/WebAI/rtai/../models/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: system memory used  = 4165.48 MiB\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 76.19 MiB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n8 threads: 2 min 5 sec\\n4 threads: 1 min 37 sec\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from os import getcwd\n",
    "\n",
    "# llm = Llama(model_path=\"%s/../models/mistral-7b-instruct-v0.1.Q5_K_S.gguf\" % getcwd(), n_threads=4)\n",
    "llm = Llama(model_path=\"%s/../models/mistral-7b-instruct-v0.1.Q4_K_M.gguf\" % getcwd())\n",
    "\"\"\"\n",
    "8 threads: 2 min 5 sec\n",
    "4 threads: 1 min 37 sec\n",
    "\"\"\"\n",
    "# llm = Llama(model_path=\"%s/../models/llama-2-7b.Q4_K_M.gguf\" % getcwd(), n_threads=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-83d4d909-4f45-4f84-851e-f3e0ffb3f16e', 'object': 'text_completion', 'created': 1703390409, 'model': '/Users/jfeibs/JRF/repos/WebAI/rtai/../models/mistral-7b-instruct-v0.1.Q4_K_M.gguf', 'choices': [{'text': ' crust:\\n\\nIngredients:\\n- 3 cups all-purpose flour\\n- 1 package active dry yeast\\n- 2 tablespoons olive oil\\n- 2 teaspoons salt\\n- 1 cup warm water\\n\\nInstructions:\\n1. Begin by combining the flour, yeast, salt and olive oil in a large bowl. Mix everything together until it forms a rough dough.\\n\\n2. Slowly pour in the warm water while stirring the dough. Be sure to mix everything well, so that there are no lumps remaining. \\n\\n3. Knead the dough on a floured surface for about 10 minutes. This will help develop gluten and make the crust more elastic. If the dough is too sticky, add more flour as needed.\\n\\n4. Place the dough in a greased bowl, cover with a clean cloth or plastic wrap, and allow it to rise in a warm, draft-free environment for about 1 hour. This will allow the yeast to ferment and produce carbon dioxide, causing the dough to expand. \\n\\n5. Once the dough has risen, gently punch it down and divide it into the desired number of portions. Roll out each portion on a floured surface until it reaches your desired thickness.\\n\\n6. Preheat your oven to the highest temperature possible, usually around 500째F or 260째C. Place the pizza crust on a greased or parchment-lined baking sheet or pizza stone.\\n\\n7. Add your favorite toppings and bake for 10-15 minutes, until the crust is golden brown and the cheese is melted and bubbly.\\n\\n8. Remove from oven, let it cool for a few minutes, slice and serve hot. Enjoy!', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 13, 'completion_tokens': 393, 'total_tokens': 406}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2011.22 ms\n",
      "llama_print_timings:      sample time =     129.41 ms /   394 runs   (    0.33 ms per token,  3044.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2010.91 ms /    13 tokens (  154.69 ms per token,     6.46 tokens per second)\n",
      "llama_print_timings:        eval time =   93242.07 ms /   393 runs   (  237.26 ms per token,     4.21 tokens per second)\n",
      "llama_print_timings:       total time =   96726.29 ms\n"
     ]
    }
   ],
   "source": [
    "answer = llm.create_completion('Write a step-by-step recipe to make a pizza', max_tokens=512)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " crust:\n",
      "\n",
      "Ingredients:\n",
      "- 3 cups all-purpose flour\n",
      "- 1 package active dry yeast\n",
      "- 2 tablespoons olive oil\n",
      "- 2 teaspoons salt\n",
      "- 1 cup warm water\n",
      "\n",
      "Instructions:\n",
      "1. Begin by combining the flour, yeast, salt and olive oil in a large bowl. Mix everything together until it forms a rough dough.\n",
      "\n",
      "2. Slowly pour in the warm water while stirring the dough. Be sure to mix everything well, so that there are no lumps remaining. \n",
      "\n",
      "3. Knead the dough on a floured surface for about 10 minutes. This will help develop gluten and make the crust more elastic. If the dough is too sticky, add more flour as needed.\n",
      "\n",
      "4. Place the dough in a greased bowl, cover with a clean cloth or plastic wrap, and allow it to rise in a warm, draft-free environment for about 1 hour. This will allow the yeast to ferment and produce carbon dioxide, causing the dough to expand. \n",
      "\n",
      "5. Once the dough has risen, gently punch it down and divide it into the desired number of portions. Roll out each portion on a floured surface until it reaches your desired thickness.\n",
      "\n",
      "6. Preheat your oven to the highest temperature possible, usually around 500째F or 260째C. Place the pizza crust on a greased or parchment-lined baking sheet or pizza stone.\n",
      "\n",
      "7. Add your favorite toppings and bake for 10-15 minutes, until the crust is golden brown and the cheese is melted and bubbly.\n",
      "\n",
      "8. Remove from oven, let it cool for a few minutes, slice and serve hot. Enjoy!\n"
     ]
    }
   ],
   "source": [
    "print(answer['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"1\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        print\n",
    "        str\n",
    "        \"\"\"\n",
    "        return \"2\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        display\n",
    "        \"\"\"\n",
    "        return \"3\"\n",
    "    \n",
    "t = Tester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tester' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tester' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "t.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./rtai'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from os import environ\n",
    "load_dotenv()\n",
    "environ['RTAI_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
